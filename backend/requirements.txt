fastapi==0.112.2
uvicorn[standard]==0.30.6
SQLAlchemy==2.0.34
pydantic==2.9.2
pydantic-settings==2.4.0
python-multipart==0.0.9
httpx==0.27.2
transformers==4.43.3
torch==2.2.2
# Optional: local inference without server (comment out if not needed)
# llama-cpp-python==0.2.90
